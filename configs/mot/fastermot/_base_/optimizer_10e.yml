epoch: 10

# LearningRate:
#   base_lr: 0.01
#   schedulers:
#   - !PiecewiseDecay
#     gamma: 0.1
#     milestones: [8, 10]
#   - !LinearWarmup
#     start_factor: 0.2
#     steps: 500

# OptimizerBuilder:
#   optimizer:
#     momentum: 0.9
#     type: Momentum
#   regularizer:
#     factor: 0.0001
#     type: L2


LearningRate:
  base_lr: 0.001
  schedulers:
  - !PiecewiseDecay
    gamma: 0.1
    milestones: [8,]
    use_warmup: False

OptimizerBuilder:
  optimizer:
    type: Adam
  regularizer: NULL
